{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-16T07:45:25.305155Z","iopub.execute_input":"2023-03-16T07:45:25.305602Z","iopub.status.idle":"2023-03-16T07:45:25.312439Z","shell.execute_reply.started":"2023-03-16T07:45:25.305554Z","shell.execute_reply":"2023-03-16T07:45:25.311213Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Level 1","metadata":{}},{"cell_type":"code","source":"#1. Write a Python code to import a CSV file and read the first 5 rows of data.\n\n# Change the file path to the location of your CSV file\nfile_path = 'path/to/your/file.csv'\n\n# Use pandas to read the CSV file into a DataFrame\ndata_frame = pd.read_csv(file_path)\n\n# Print the first 5 rows of the DataFrame\nprint(data_frame.head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Python function to calculate the mean of a given list of numbers:\ndef mean(numbers):\n    if len(numbers) == 0:\n        return None\n    return sum(numbers) / len(numbers)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T08:17:30.966702Z","iopub.execute_input":"2023-03-16T08:17:30.967136Z","iopub.status.idle":"2023-03-16T08:17:30.973020Z","shell.execute_reply.started":"2023-03-16T08:17:30.967098Z","shell.execute_reply":"2023-03-16T08:17:30.971548Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 3. Python code to remove all duplicate values from a given list:\n\ndef remove_duplicates(lst):\n    return list(set(lst))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4. sort a list of integers in ascending order:\ndef sort_numbers(numbers):\n    return sorted(numbers)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-16T08:18:34.881329Z","iopub.execute_input":"2023-03-16T08:18:34.881772Z","iopub.status.idle":"2023-03-16T08:18:34.886681Z","shell.execute_reply.started":"2023-03-16T08:18:34.881733Z","shell.execute_reply":"2023-03-16T08:18:34.885453Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 5. find the maximum value in a given list:\ndef find_maximum(lst):\n    if len(lst) == 0:\n        return None\n    return max(lst)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. concatenate two given lists:\ndef concatenate_lists(lst1, lst2):\n    return lst1 + lst2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7. calculate the standard deviation of a given list of numbers\nimport math\n\ndef standard_deviation(numbers):\n    if len(numbers) == 0:\n        return None\n    mean = sum(numbers) / len(numbers)\n    variance = sum([((num - mean) ** 2) for num in numbers]) / len(numbers)\n    return math.sqrt(variance)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 8. create a histogram of a given list of numbers:\nimport matplotlib.pyplot as plt\n\ndef create_histogram(numbers):\n    plt.hist(numbers)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-16T08:20:10.701769Z","iopub.execute_input":"2023-03-16T08:20:10.702449Z","iopub.status.idle":"2023-03-16T08:20:10.707885Z","shell.execute_reply.started":"2023-03-16T08:20:10.702406Z","shell.execute_reply":"2023-03-16T08:20:10.706706Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 9. find the median value of a given list of numbers:\ndef find_median(lst):\n    n = len(lst)\n    s = sorted(lst)\n    return (s[n // 2] if n % 2 != 0 else (s[n // 2 - 1] + s[n // 2]) / 2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 10. linear regression on a given dataset using the scikit-learn library:\nfrom sklearn.linear_model import LinearRegression\n\n# Assuming the dataset is in two lists, x and y\ndef perform_linear_regression(x, y):\n    model = LinearRegression().fit([[xi] for xi in x], y)\n    return model.coef_, model.intercept_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# level 2\n\nWrite a Python code to read a JSON file and extract data using pandas library.\n\nWrite a Python code to perform a one-sample t-test on a given dataset.\n\nWrite a Python code to perform a chi-square test on a given contingency table.\n\nWrite a Python code to perform a K-Means clustering on a given dataset using the scikit-learn library.\n\nWrite a Python code to perform a principal component analysis (PCA) on a given dataset using the scikit-learn library.\n\nWrite a Python code to perform a linear regression on a given dataset using the statsmodels library.\n\nWrite a Python code to perform a logistic regression on a given dataset using the statsmodels library.\n\nWrite a Python code to create a heat map of a given dataset using the seaborn library.\n\nWrite a Python code to create a scatter plot of a given dataset using the matplotlib library.\n\nWrite a Python code to calculate the correlation coefficient between two variables in a given dataset.\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# 11. Code to read a JSON file and extract data using pandas library:\n\n\n# read the JSON file using pandas\ndf = pd.read_json('filename.json')\n\n# print the dataframe\nprint(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12. perform a one-sample t-test on a given dataset:\n\nimport scipy.stats as stats\n\n# assume that data is already loaded into a variable called data\n\n# perform the t-test and print the results\nt_stat, p_value = stats.ttest_1samp(data, population_mean)\nprint(\"t-statistic:\", t_stat)\nprint(\"p-value:\", p_value)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 13. perform a chi-square test on a given contingency table:\n\nimport scipy.stats as stats\nimport numpy as np\n\n# assume that contingency table is already loaded into a variable called contingency_table\n\n# calculate the expected frequencies\nrow_sums = contingency_table.sum(axis=1)\ncol_sums = contingency_table.sum(axis=0)\ntotal_sum = contingency_table.sum()\nexpected = np.outer(row_sums, col_sums) / total_sum\n\n# perform the chi-square test and print the results\nchi_squared_statistic = ((contingency_table - expected)**2 / expected).sum().sum()\ndegrees_of_freedom = (contingency_table.shape[0] - 1) * (contingency_table.shape[1] - 1)\np_value = 1 - stats.chi2.cdf(chi_squared_statistic, degrees_of_freedom)\nprint(\"chi-squared statistic:\", chi_squared_statistic)\nprint(\"p-value:\", p_value)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 14. perform a K-Means clustering on a given dataset using the scikit-learn library:\n\nfrom sklearn.cluster import KMeans\nimport pandas as pd\n\n# assume that data is already loaded into a variable called data\n\n# perform k-means clustering with k clusters and fit the model to the data\nkmeans = KMeans(n_clusters=k, random_state=0).fit(data)\n\n# add the cluster labels to the original data\ndata_with_clusters = pd.concat([data, pd.Series(kmeans.labels_, name='Cluster')], axis=1)\n\n# print the cluster centers\nprint(\"Cluster centers:\")\nprint(kmeans.cluster_centers_)\n\n# print the data with cluster labels\nprint(\"Data with cluster labels:\")\nprint(data_with_clusters)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 15. Performing Principal Component Analysis (PCA) using scikit-learn library:\n\n# Importing necessary libraries\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\n# Loading dataset\niris = load_iris()\nX = iris.data\n\n# Creating a PCA instance with 2 components\npca = PCA(n_components=2)\n\n# Fitting and transforming the data\nX_pca = pca.fit_transform(X)\n\n# Printing the first 5 rows of the transformed data\nprint(X_pca[:5])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#16. Performing Linear Regression using statsmodels library:\n\n# Importing necessary libraries\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Loading dataset\ndata = pd.read_csv('data.csv')\nX = data[['feature1', 'feature2', 'feature3']]\ny = data['target']\n\n# Adding a constant term to the predictor variable\nX = sm.add_constant(X)\n\n# Fitting the model\nmodel = sm.OLS(y, X).fit()\n\n# Printing the summary of the model\nprint(model.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#17. Performing Logistic Regression using statsmodels library:\n\n# Importing necessary libraries\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Loading dataset\ndata = pd.read_csv('data.csv')\nX = data[['feature1', 'feature2', 'feature3']]\ny = data['target']\n\n# Adding a constant term to the predictor variable\nX = sm.add_constant(X)\n\n# Fitting the model\nmodel = sm.Logit(y, X).fit()\n\n# Printing the summary of the model\nprint(model.summary())\n# Note: In the logistic regression code, sm.Logit() function is used instead of sm.OLS() to perform logistic regression.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 18. To create a heat map of a given dataset using the seaborn library:\nimport seaborn as sns\nimport pandas as pd\n\n# Load the dataset\ndata = pd.read_csv('your_dataset.csv')\n\n# Create a heatmap using seaborn\nsns.heatmap(data.corr(), annot=True, cmap='coolwarm')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 19. To create a scatter plot of a given dataset using the matplotlib library:\npython\nCopy code\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load the dataset\ndata = pd.read_csv('your_dataset.csv')\n\n# Create a scatter plot using matplotlib\nplt.scatter(data['x'], data['y'])\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter plot of x vs. y')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 20. To calculate the correlation coefficient between two variables in a given dataset:\nimport pandas as pd\n\n# Load the dataset\ndata = pd.read_csv('your_dataset.csv')\n\n# Calculate the correlation coefficient between x and y\ncorr_coeff = data['x'].corr(data['y'])\n\nprint('The correlation coefficient between x and y is:', corr_coeff)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 21. Creating a Deep Neural Network with Keras:\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Create a sequential model\nmodel = Sequential()\n\n# Add the first hidden layer with 32 neurons and input shape of (num_features,)\nmodel.add(Dense(32, input_shape=(num_features,), activation='relu'))\n\n# Add additional hidden layers\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\n\n# Add the output layer with a single neuron and a sigmoid activation function\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model with binary cross-entropy loss and the Adam optimizer\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 22. Creating a Convolutional Neural Network with TensorFlow:\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Create a sequential model\nmodel = Sequential()\n\n# Add the convolutional layers\nmodel.add(Conv2D(32, (3,3), activation='relu', input_shape=(image_height, image_width, num_channels)))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(64, (3,3), activation='relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(128, (3,3), activation='relu'))\n\n# Add the fully connected layers\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\n# Compile the model with categorical cross-entropy loss and the Adam optimizer\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 23. Creating a Recurrent Neural Network with TensorFlow:\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\n# Create a sequential model\nmodel = Sequential()\n\n# Add the LSTM layer with 32 units\nmodel.add(LSTM(32, input_shape=(timesteps, num_features)))\n\n# Add the output layer with a single neuron and a sigmoid activation function\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model with binary cross-entropy loss and the Adam optimizer\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 24. Performing Hyperparameter Tuning with scikit-learn:\n\nimport numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define the hyperparameter space to search over\nparam_dist = {'n_estimators': [10, 50, 100, 200],\n              'max_depth': [None, 10, 20, 30],\n              'min_samples_split': [2, 5, 10],\n              'min_samples_leaf': [1, 2, 4]}\n\n# Create a random forest classifier\nrf = RandomForestClassifier()\n\n# Perform a random search over the hyperparameter space\nrandom_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=10)\n\n# Fit the random search to the training data\nrandom_search.fit(X_train, y_train)\n\n# Print the best hyperparameters and their corresponding score\nprint(\"Best Hyperparameters:\", random_search.best_params_)\nprint(\"Best Score:\", random_search.best_score_)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 25. Performing Model Selection with scikit-learn:\n\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define the models to search over\nmodels = {'SVM': SVC(),\n          'Random Forest': RandomForestClassifier()}\n\n# Define the hyperparameter space for each model\nparam_grids = {'SVM': {'C': [0.1, 1, 10, 100],\n                       'gamma': [0.1, 1, 10, 100]},\n               'Random Forest': {'n_estimators': [10, 50, 100, 200],\n                                 'max_depth': [None, 10, 20, 30]}}\n\n# Perform a grid search over the models and hyperparameters\ngrid_search = GridSearchCV(estimator=models, param_grid=param_grids)\n\n# Fit the grid search to the training data\ngrid_search.fit(X_train, y_train)\n\n# Print the best model and its corresponding hyperparameters\nprint(\"Best Model:\", grid_search.best_estimator_)\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\nprint(\"Best Score:\", grid_search.best_score_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 26. Performing Data Augmentation with TensorFlow:\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define the data augmentation parameters\ndatagen = ImageDataGenerator(rotation_range=10,\n                             width_shift_range=0.1,\n                             height_shift_range=0.1,\n                             shear_range=0.1,\n                             zoom_range=0.1,\n                             horizontal_flip=True,\n                             fill_mode='nearest')\n\n# Generate augmented images from the training set\naugmented_data = datagen.flow(X_train, y_train, batch_size=32)\n\n# Train the model on the augmented data\nmodel.fit(augmented_data, epochs=10, validation_data=(X_test, y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 27. Implementing a Generative Adversarial Network (GAN) with TensorFlow:\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load MNIST dataset\n(x_train, _), (_, _) = mnist.load_data()\n\n# Normalize data\nx_train = x_train / 127.5 - 1.\nx_train = np.expand_dims(x_train, axis=3)\n\n# Define generator model\ngenerator = Sequential([\n    Dense(256, input_shape=(100,)),\n    LeakyReLU(alpha=0.2),\n    Dense(512),\n    LeakyReLU(alpha=0.2),\n    Dense(1024),\n    LeakyReLU(alpha=0.2),\n    Dense(28 * 28 * 1, activation='tanh'),\n    Reshape((28, 28, 1))\n])\n\n# Define discriminator model\ndiscriminator = Sequential([\n    Flatten(input_shape=(28, 28, 1)),\n    Dense(512),\n    LeakyReLU(alpha=0.2),\n    Dense(256),\n    LeakyReLU(alpha=0.2),\n    Dense(1, activation='sigmoid')\n])\n\n# Compile discriminator model\ndiscriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n\n# Combine generator and discriminator models into a GAN model\ngan = Sequential([\n    generator,\n    discriminator\n])\n\n# Compile GAN model\ngan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n\n# Define function to generate images using the generator model\ndef generate_images(generator, epoch, latent_dim):\n    noise = np.random.normal(0, 1, (25, latent_dim))\n    gen_imgs = generator.predict(noise)\n\n    # Rescale images to 0-1 range\n    gen_imgs = 0.5 * gen_imgs + 0.5\n\n    fig, axs = plt.subplots(5, 5)\n    cnt = 0\n    for i in range(5):\n        for j in range(5):\n            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n            axs[i,j].axis('off')\n            cnt += 1\n    fig.savefig(f\"gan_images_epoch_{epoch}.png\")\n    plt.close()\n\n# Define training hyperparameters\nepochs = 10000\nbatch_size = 128\nlatent_dim = 100\n\n# Define labels for real and fake images\nreal_labels = np.ones((batch_size, 1))\nfake_labels = np.zeros((batch_size, 1))\n\n# Train GAN model\nfor epoch in range(epochs):\n    # Train discriminator on real images\n    idx = np.random.randint(0, x_train.shape[0], batch_size)\n    real_imgs = x_train[idx]\n    d_loss_real = discriminator.train_on_batch(real_imgs, real_labels)\n\n    # Train discriminator on fake images generated by generator\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    fake_imgs = generator.predict(noise)\n    d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_labels)\n\n    # Train generator by fooling the discriminator\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    g_loss = gan.train_on_batch(noise, real_labels)\n\n    # Print training progress\n    print(f\"Epoch: {epoch}, D_loss_real: {d_loss_real}, D_loss_fake:\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 28. perform natural language processing (NLP) on a given text dataset using the NLTK library.\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.probability import FreqDist\nimport string\n\n# Download necessary NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n# Load text dataset\ntext = \"Insert your text dataset here\"\n\n# Tokenize text into words\nwords = word_tokenize(text)\n\n# Remove stop words and punctuation\nstop_words = set(stopwords.words('english') + list(string.punctuation))\nfiltered_words = [word.lower() for word in words if word.lower() not in stop_words]\n\n# Lemmatize filtered words\nlemmatizer = WordNetLemmatizer()\nlemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n\n# Print frequency distribution of lemmatized words\nfreq_dist = FreqDist(lemmatized_words)\nprint(freq_dist.most_common(10))\n\n# Tokenize text into sentences\nsentences = sent_tokenize(text)\n\n# Perform part-of-speech (POS) tagging on the first sentence\ntagged_words = nltk.pos_tag(word_tokenize(sentences[0]))\nprint(tagged_words)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 29. create a time series forecasting model for a given dataset using the Prophet library.\nimport pandas as pd\nfrom fbprophet import Prophet\n\n# Load dataset into a pandas DataFrame\ndf = pd.read_csv('your_dataset.csv')\n\n# Rename columns to 'ds' and 'y'\ndf = df.rename(columns={'date': 'ds', 'value': 'y'})\n\n# Create Prophet model and fit to data\nmodel = Prophet()\nmodel.fit(df)\n\n# Generate future dataframe with 365 days of daily frequency\nfuture = model.make_future_dataframe(periods=365, freq='D')\n\n# Make predictions for future dataframe\nforecast = model.predict(future)\n\n# Plot the forecast\nfig = model.plot(forecast)\n\nThis code performs the following steps:\n\n# Load the dataset into a pandas DataFrame using pd.read_csv().\n# Rename the columns to 'ds' and 'y' to match the input format required by Prophet.\n# Create a Prophet model using Prophet() and fit it to the data using model.fit(df).\n# Generate a future dataframe with 365 days of daily frequency using model.make_future_dataframe(periods=365, freq='D').\n# Make predictions for the future dataframe using model.predict(future).\n# Plot the forecast using model.plot(forecast).\n# Note that Prophet also allows you to specify additional parameters such as seasonality, holidays, and changepoints, which can be used to improve the accuracy of the forecast.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 30. perform reinforcement learning for a given problem using the OpenAI gym library.\n# Let's consider the classic \"CartPole\" problem, where we have to balance a pole on top of a cart that can move left or right. We'll use the Q-learning algorithm for this problem.\n\n# First, let's install the OpenAI gym library:\n!pip install gym\n\n# Now, let's import the necessary libraries and create the environment:\nimport gym\nimport random\n\nenv = gym.make(\"CartPole-v1\")\n# The env variable is an instance of the environment we'll be working with.\n# We can get some information about the environment by calling the env methods, for example:\n\nprint(f\"Observation space: {env.observation_space}\")\nprint(f\"Action space: {env.action_space}\")\n\n# The output of this code will be:\n\n# Observation space: Box(4,)\n# Action space: Discrete(2)\n# The observation space is a 4-dimensional continuous space, which represents the state of the cart-pole system.\n# The action space is a discrete space with two possible actions: move left or move right.\n\n# Now, let's define the Q-learning algorithm. We'll create a Q-table that will store the expected reward for each possible state-action pair. We'll start with a random Q-table, and we'll update it based on the rewards we get during the training process.\n\n# Initialize Q-table\nq_table = {}\nfor i in range(env.observation_space.shape[0]):\n    for j in range(env.action_space.n):\n        q_table[(i, j)] = random.uniform(-1, 1)\n\n# Hyperparameters\nalpha = 0.1\ngamma = 0.99\nepsilon = 0.1\nnum_episodes = 10000\n# In this code, we initialize the Q-table with random values between -1 and 1. We also define some hyperparameters for the algorithm, such as the learning rate (alpha), the discount factor (gamma), and the exploration rate (epsilon). Finally, we define the number of episodes we'll train the agent on.\n\n# Now, let's train the agent:\n\nfor episode in range(num_episodes):\n    state = env.reset()\n    done = False\n    \n    while not done:\n        # Choose action\n        if random.uniform(0, 1) < epsilon:\n            action = env.action_space.sample() # Exploration\n        else:\n            q_values = [q_table[(state, a)] for a in range(env.action_space.n)]\n            action = q_values.index(max(q_values)) # Exploitation\n        \n        # Take action\n        next_state, reward, done, info = env.step(action)\n        \n        # Update Q-table\n        q_table[(state, action)] += alpha * (reward + gamma * max([q_table[(next_state, a)] for a in range(env.action_space.n)]) - q_table[(state, action)])\n        \n        state = next_state\n    \n    if episode % 100 == 0:\n        print(f\"Episode {episode}\")\n# In this code, we loop over the episodes and for each episode, we reset the environment to its initial state (env.reset()) and set done to False. We then loop over the environment steps until done is True.\n# In each step, we choose an action based on the current state and the Q-table (if random.uniform(0, 1) < epsilon: is the exploration step and else: is the exploitation step). \n# We then take the chosen action (env.step(action)) and get the next state,\n# reward, done, and info. We then update the Q-table based on the reward we got and the expected reward for the next state (q_table[(state, action)] += alpha * (reward + gamma * max([q_table[(next_state, a)] for a in range(env.action_space.n)]) - q_table[(state, action)])). Finally, we set the current state to the next state.\n# We also print the episode number every 100 episodes (if episode % 100 == 0:).\n# After training the agent, we can test it by letting it play the game using the Q-table:\n\nstate = env.reset()\ndone = False\n\nwhile not done:\n    q_values = [q_table[(state, a)] for a in range(env.action_space.n)]\n    action = q_values.index(max(q_values))\n    state, reward, done, info = env.step(action)\n    env.render()\n\nenv.close()\n# In this code, we reset the environment and set done to False. We then loop over the environment steps until done is True. In each step, we choose the action with the highest expected reward based on the Q-table (action = q_values.index(max(q_values))). We then take the chosen action (env.step(action)) and get the next state, reward, done, and info. We also render the environment to visualize the agent playing the game. Finally, we close the environment.\n\n# That's it! This is an example of how to perform reinforcement learning using the OpenAI gym library. Of course, this is just a simple example, and there are many more things you can do with the gym library, such as using different algorithms, modifying the environment, and so on.","metadata":{},"execution_count":null,"outputs":[]}]}